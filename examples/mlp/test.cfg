[DEFAULT]
model_dir=test1
model=MLP
onehot=classes.txt
loss=softmax_cross_entropy
metrics=accuracy,auc
hidden_size=50
output_size=10

[adagrad]
model_dir=test2
onehot=True
optimizer=AdagradOptimizer
learning_rate=0.001
optimizer_kwargs={"initial_accumulator_value": 0.01}

[expdecay]
model_dir=test3
onehot=True
optimizer=AdagradOptimizer
learning_rate=exponential_decay
learning_rate_kwargs={
    "learning_rate": 0.01,
    "decay_steps": 1000,
    "decay_rate": 0.9}
